#!/usr/bin/env python
# encoding: utf-8
"""
Scrape CAS for all of the needed tables and populate the local MongoDB.

"""

import os
import logging
logging.basicConfig(level=logging.INFO)

import numpy as np
import pyfits

import pymongo

import casjobs

# Temporary file names.
_local_tmp_dir = os.path.join(os.environ.get("SDSS_LOCAL", "."), ".sdss")
_fields_file   = os.path.join(_local_tmp_dir, "fields.{0}.fits")
_stars_file    = os.path.join(_local_tmp_dir, "stars.fits")

# Connect to database.
_db_server = os.environ.get("MONGO_SERVER", "localhost")
_db_port   = int(os.environ.get("MONGO_PORT", 27017))
_db_name   = os.environ.get("MONGO_DB", "sdss")
_db = pymongo.Connection(_db_server, _db_port)[_db_name]

# Collection names.
_fields_collection = "fields"
_stars_collection  = "stars"

def clear_fields():
    _db.drop_collection(_fields_collection)
    _db.drop_collection(_stars_collection)

def populate_fields(rng):
    """
    Populate the fields collection using the temporary files generated by
    `get_field_table`.

    ## Arguments

    * `rng` (list): The indices of the temporary files to use.

    """
    # Access the fields collection.
    coll = _db[_fields_collection]

    # Loop over the temporary files.
    rng = np.atleast_1d(rng)
    for i in rng:
        logging.info("Pushing table from %s to db"%(_fields_file.format(i)))

        hdus = pyfits.open(_fields_file.format(i))
        names = hdus[1].data.dtype.names

        # Loop over the rows in the table.
        for i, row in enumerate(np.array(hdus[1].data)):
            doc = dict(zip(names, row.tolist()))
            doc["_id"] = doc.pop("fieldID")
            coll.insert(doc)

def populate_stars():
    """
    Populate the stars collection using the temporary files generated by
    `get_star_table`.

    """
    # Access the fields collection.
    coll = _db[_stars_collection]

    hdus = pyfits.open(_stars_file)
    names = hdus[1].data.dtype.names

    # Loop over the rows in the table.
    for i, row in enumerate(np.array(hdus[1].data)):
        doc = dict(zip(names, row.tolist()))
        doc["_id"] = doc.pop("objID")
        coll.insert(doc)

def get_field_table():
    """
    Query CAS to get all the fields in Stripe 82.

    ## Returns

    * `i` (int): The number of temporary files saved.

    """
    # Get the fields.
    field_table = "fields"
    jobs = casjobs.CasJobs()

    # How many fields are there?
    field_count = jobs.count("FROM Stripe82..Field")

    # MAGIC: An approximation of the maximum number of fields that can be
    # collected without filling MyDB.
    max_rows = 80000

    for i, row in enumerate(range(0, field_count+max_rows+1, max_rows)):
        logging.info("Querying CAS for rows %d to %d"%(row+1, row+max_rows))

        q = """SELECT * INTO mydb.{0} FROM
(SELECT ROW_NUMBER() OVER (ORDER BY fieldID ASC) AS ROW_NUMBER, *
    FROM Stripe82..Field) foo
WHERE ROW_NUMBER BETWEEN {1} AND {2}
""".format(field_table, row+1, row+max_rows)
        try:
            jobs.drop_table(field_table)
        except:
            pass
        job_id = jobs.submit(q)
        status = jobs.monitor(job_id)
        if status[0] != 5:
            raise Exception("Couldn't complete field list request.")

        # Download the output file.
        logging.info("Downloading file to %s"%(_fields_file.format(i)))
        jobs.request_and_get_output(field_table, "FITS", _fields_file.format(i))

    jobs.drop_table(field_table)
    return i

def get_star_table():
    """
    Query CAS to get all the stars in the Stripe 82 co-adds.

    NOTE: This only gets the stars in the Pisces overdensity for now.

    """
    q = """SELECT p.objID,p.ra,p.dec,p.u,p.g,p.r,p.i,p.z INTO mydb.{0}
FROM Stripe82..PhotoPrimary p
WHERE p.type = 6 AND p.g BETWEEN 14. AND 28.
AND (p.run = 106 OR p.run = 206)
AND (p.flags &
    (dbo.fPhotoFlags('BRIGHT')+dbo.fPhotoFlags('EDGE')+dbo.fPhotoFlags('BLENDED')
    +dbo.fPhotoFlags('SATURATED')+dbo.fPhotoFlags('NOTCHECKED')
    +dbo.fPhotoFlags('NODEBLEND')+dbo.fPhotoFlags('INTERP_CENTER')
    +dbo.fPhotoFlags('DEBLEND_NOPEAK')+dbo.fPhotoFlags('PEAKCENTER'))) = 0
AND p.ra BETWEEN 350 AND 360
""".format(star_table, conditions)

    star_table = "stars"
    jobs = casjobs.CasJobs()

    try:
        jobs.drop_table(star_table)
    except:
        pass

    job_id = jobs.submit(q)
    status = jobs.monitor(job_id)
    if status[0] != 5:
        raise Exception("Couldn't complete star list request.")

    # Download the output file.
    logging.info("Downloading file to %s"%(_stars_file))
    jobs.request_and_get_output(star_table, "FITS", _stars_file)

def post_process():
    """
    Wrap the R.A. values properly and index the stars spherically.

    """
    code  = "db[{0}].find().forEach( function (obj) {"\
            "    while (obj.ramin > 180) obj.ramin -= 360.;"\
            "    while (obj.ramax > 180) obj.ramax -= 360.;"\
            "    if (obj.ramin > obj.ramax) {"\
            "        var tmp = obj.ramin;"\
            "        obj.ramin = obj.ramax;"\
            "        obj.ramax = obj.ramin;"\
            "    }"\
            "    db[{0}].save(obj);"\
            "})"
    _db.eval(code.format(_fields_collection))

    # Wrap and process the stars too.
    code = """
function() {{ db.{0}.find().forEach( function (obj) {{
        while (obj.ra > 180) obj.ra -= 360.;
        //obj.coords =
        db.{0}.save(obj);
    }});
}}"""
    _db.eval(code.format(_stars_collection))

if __name__ == "__main__":
    import sys

    populate_stars()
    sys.exit(0)

    if "--test" in sys.argv:
        populate_fields(0)
        sys.exit(0)

    if "--clobber" in sys.argv:
        clear_fields()

    if "--get" in sys.argv:
        nfields = get_field_table()
    else:
        nfields = int(sys.argv[sys.argv.index("-n")+1])

    populate_fields(range(nfields))

